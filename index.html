
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
    <script src="js/google-code-prettify/prettify.js"></script> -->
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>Triplane Meets Gaussian Splatting: <br> Fast and Generalizable Single-View 3D Reconstruction with Transformers</h2>
            <h4 style="color:#5a6268;">Arxiv 2023</h4>
            <hr>
            <h6 style="line-height: 26px;">
              <a href="" target="_blank">Zi-Xin Zou</a><sup>1</sup>,
              <a href="" target="_blank">Zhipeng Yu</a><sup>2</sup>,
              <a href="" target="_blank">Yuan-Chen Guo</a><sup>1,3*</sup>,
              <a href="" target="_blank">Yangguang Li</a><sup>3</sup>,
              <a href="" target="_blank">Ding Liang</a><sup>3</sup>,
              <a href="" target="_blank">Yan-Pei Cao</a><sup>3,†</sup>,
              <a href="" target="_blank">Song-Hai Zhang</a><sup>1,†</sup>,
            </h6>
          <p>
              <sup>1</sup>BNRist, Tsinghua University &nbsp;&nbsp;
              <sup>2</sup>University of the Chinese Academy of Sciences &nbsp;&nbsp;
              <sup>3</sup>VAST
              <br>
              <sup>*</sup>Intern at VAST &nbsp;&nbsp;
              <sup>†</sup>Corresponding Authors
          </p>

          <div class="row justify-content-center">
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <!-- <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                  <i class="fa fa-github-alt"></i> Code </a> </p>
            </div> -->
            <!-- <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EjYHbCBnV-VPjBqNHdNulIABq9sYAEpSz4NPLDI72a85vw" role="button"  target="_blank">
                  <i class="fa fa-database"></i> Model </a> </p>
            </div>
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://huggingface.co/spaces/liuyuan-pal/SyncDreamer" role="button"  target="_blank">
                  <i class="fa fa-desktop"></i> Live Demo </a> </p>
            </div> -->
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h3>Abstract</h3>
                <hr style="margin-top:0px">
                <h6 style="color:#8899a5" class="text-left"> 
                  TGS enables fast reconstruction from single-view image. It builds the 3D representation upon a hybrid Triplane-Gaussian representation by evaluating a transformer-based framework, from which 3D Gaussians would be decoded.
                </h6>
    
                  <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                      <source src="video/teaser.mp4" type="video/mp4">
                  </video>
    
                  <br>
    
              <p class="text-left">
                Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques.
              </p>
            </div>
          </div>
        </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h2>Method</h2>
              <hr style="margin-top:0px">
              <img src="assets/TGS-framework-v2.png" alt="Overview" width="100%">
              <p class="text-left">
                The overview of our framework. Given an image with its camera parameters, we first encode them into a set of latent feature tokens by leveraging a pre-trained ViT model. Our two transformer-based networks, point cloud decoder and triplane decoder, take initial positional embedding as input and project image tokens onto latent feature tokens of respective 3D representation via cross-attention. Subsequently, a point cloud and a triplane can be de-tokenized from the output of decoders, respectively. After the point cloud decoder, we adapt a point upsampling module with condition-aware projection to densify the point cloud. Additionally, we utilize a geometryaware encoding to project point cloud features into the initial positional embedding of triplane latent. Finally, 3D Gaussians are decoded by the point cloud, the triplane features and image features for novel view rendering.
              </p>
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h2>Comparison on the GSO dataset</h2>
                <hr style="margin-top:0px">
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/gso_comparison.mp4" type="video/mp4">
                </video>
              <p class="text-center">
                Qualitative comparison between Zero-1-2-3, One-2-3-45 and Ours on GSO (Google Scanned Object) dataset.
              </p>
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h2>More results</h2>
                <hr style="margin-top:0px">
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/more_gso.mp4" type="video/mp4">
                </video>
                <p class="text-center">
                  More results from GSO dataset.
                </p>
            </div>
          </div>
        </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h2>Ablation</h2>
              <hr style="margin-top:0px">
              <img src="assets/ab-repr.png" alt="ablation-representation" width="100%">
              <p class="text-center">
                Qualitative comparison between different 3D representations: (1) naive generalizable 3D Gaussian (3DG), (2) Triplane-NeRF and (3) Triplane-Guassian.
              </p>
              <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="video/ab.mp4" type="video/mp4">
              </video>
              <img src="assets/ab-method.png" alt="ablation-method" width="100%">
              <p class="text-center">
                Ablation experiments to assess the effect of Projection-aware Condition (P.C.) and Geometry-aware Encoding (G.E.).
              </p>
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="warp-container">
    <div class="row">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{,
    title={Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers},
    author={Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang},
    journal={arXiv preprint arXiv:xxxx},
    year={2023}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
    This website is adapted from <a href="https://liuyuan-pal.github.io/SyncDreamer/" target="_blank">SyncDreamer</a>.
  </footer>

</body>
</html>
